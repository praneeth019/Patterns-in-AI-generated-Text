{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "from pycspade.helpers import spade, print_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load spaCy and NLTK resources\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load and prepare dataset\n",
    "equal_sample_df = pd.read_csv('sampled_df_pos_tags.csv')\n",
    "\n",
    "# Balance the dataset with equal samples from human and AI sources\n",
    "human_data = equal_sample_df[equal_sample_df[\"source\"] == 0]\n",
    "ai_data = equal_sample_df[equal_sample_df[\"source\"] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample to ensure equal class distribution\n",
    "human_sampled = human_data.sample(n=20000, random_state=42)\n",
    "ai_data = ai_data.sample(n=20000, random_state=42)\n",
    "equal_sample_df = pd.concat([human_sampled, ai_data], ignore_index=True)\n",
    "equal_sample_df = equal_sample_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z',!?\\-.\\s]\", '', text)\n",
    "\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\?{2,}', '?', text)\n",
    "    text = re.sub(r'\\!{2,}', '!', text)\n",
    "    text = re.sub(r',{2,}', ',', text)\n",
    "\n",
    "    text = re.sub(r'(?<!\\s)([.,!?-])(?!\\s)', r' \\1 ', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the text column\n",
    "equal_sample_df['processed_text'] = equal_sample_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = equal_sample_df\n",
    "y = equal_sample_df['source']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=72, stratify=y)\n",
    "\n",
    "# Create separate datasets for AI and human text\n",
    "X_train_ai = X_train[X_train['source'] == 1]\n",
    "X_train_human = X_train[X_train['source'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for all unique words in the dataset\n",
    "passages_all = X_train['processed_text'].tolist()\n",
    "unique_words = set(word.lower() for passage in passages_all for word in passage.split())\n",
    "word_to_index = {word: idx for idx, word in enumerate(sorted(unique_words))}\n",
    "index_to_word = {value: key for key, value in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare AI dataset for SPADE\n",
    "passages_ai = X_train_ai['processed_text'].tolist()\n",
    "final_sentences_split_dataset_ai = []\n",
    "\n",
    "for seq_idx, passage in enumerate(passages_ai, start=1):\n",
    "    sentences = sent_tokenize(passage)\n",
    "    event_id = 1\n",
    "    for sentence in sentences:\n",
    "        words = [word_to_index[word.lower()] for word in sentence.split()]\n",
    "        final_sentences_split_dataset_ai.append([seq_idx, event_id, words])\n",
    "        event_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare human dataset for SPADE\n",
    "passages_human = X_train_human['processed_text'].tolist()\n",
    "final_sentences_split_dataset_human = []\n",
    "\n",
    "for seq_idx, passage in enumerate(passages_human, start=1):\n",
    "    sentences = sent_tokenize(passage)\n",
    "    event_id = 1\n",
    "    for sentence in sentences:\n",
    "        words = [word_to_index[word.lower()] for word in sentence.split()]\n",
    "        final_sentences_split_dataset_human.append([seq_idx, event_id, words])\n",
    "        event_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SPADE algorithm for human text\n",
    "result_human = spade(data=final_sentences_split_dataset_human, support=0.25, maxgap=1, mingap=1)\n",
    "\n",
    "# Run SPADE algorithm for AI text\n",
    "result_ai = spade(data=final_sentences_split_dataset_ai, support=0.35, maxgap=1, mingap=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequences from SPADE results for human text\n",
    "sequences_human = []\n",
    "for mined_object in result_human['mined_objects']:\n",
    "    sequences_human.append('->'.join(list(map(str, mined_object.items))))\n",
    "\n",
    "# Extract sequences from SPADE results for AI text\n",
    "sequences_ai = []\n",
    "for mined_object in result_ai['mined_objects']:\n",
    "    sequences_ai.append('->'.join(list(map(str, mined_object.items))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract numbers from sequence strings\n",
    "def extract_numbers(string):\n",
    "    numbers = re.findall(r'\\((\\d+)\\)|\\[(\\d+)\\]', string)\n",
    "    numbers = [int(num) for pair in numbers for num in pair if num]\n",
    "    return tuple(numbers)\n",
    "\n",
    "# Convert sequence strings to number tuples\n",
    "all_sequences_number_human = []\n",
    "for i in sequences_human:\n",
    "    string = extract_numbers(str(i))\n",
    "    all_sequences_number_human.append(string)\n",
    "\n",
    "all_sequences_number_ai = []\n",
    "for i in sequences_ai:\n",
    "    string = extract_numbers(str(i))\n",
    "    all_sequences_number_ai.append(string)\n",
    "\n",
    "# Combine all unique sequences\n",
    "all_sequences = set(all_sequences_number_human + all_sequences_number_ai)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate word sequences from passages\n",
    "def generate_sequences(passages, word_to_index):\n",
    "    sequences = []\n",
    "    for passage in passages:\n",
    "        sentences = sent_tokenize(passage)\n",
    "        for sentence in sentences:\n",
    "            words = [word_to_index[word.lower()] for word in sentence.split() if word.lower() in word_to_index]\n",
    "            sequences.append(tuple(words))\n",
    "    return sequences\n",
    "\n",
    "# Function to check if A is a subsequence of B\n",
    "def is_subsequence(A, B):\n",
    "    str_A = ','.join(map(str, A))\n",
    "    str_B = ','.join(map(str, B))\n",
    "    return str_B in str_A\n",
    "\n",
    "# Function to count sequence frequencies in passages\n",
    "def count_sequence_frequencies(dataset, passages, word_to_index, sequence_dict):\n",
    "    freq_vector = np.zeros((dataset.shape[0], len(sequence_dict)))\n",
    "    for passage, passage_num in zip(passages, range(0, dataset.shape[0])):\n",
    "        passage_sequences = generate_sequences([passage], word_to_index)\n",
    "        \n",
    "        for seq, seq_num in zip(sequence_dict, range(0, len(sequence_dict))):\n",
    "            for passage_seq in passage_sequences:\n",
    "                if is_subsequence(seq, passage_seq):\n",
    "                    freq_vector[passage_num, seq_num] += 1\n",
    "    return freq_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency vectors for training and testing data\n",
    "passages_train = X_train['processed_text'].tolist()\n",
    "train_frequency_vectors = count_sequence_frequencies(X_train, passages_train, word_to_index, all_sequences)\n",
    "\n",
    "passages_test = X_test['processed_text'].tolist()\n",
    "test_frequency_vectors = count_sequence_frequencies(X_test, passages_test, word_to_index, all_sequences)\n",
    "\n",
    "# Convert frequency vectors to DataFrames\n",
    "df_train = pd.DataFrame(train_frequency_vectors, columns=[f\"seq_{i}\" for i in range(len(all_sequences))])\n",
    "df_test = pd.DataFrame(test_frequency_vectors, columns=[f\"seq_{i}\" for i in range(len(all_sequences))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(df_train, y_train)\n",
    "lr_pred = lr_model.predict(df_test)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, lr_pred):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate SVM with linear kernel\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(df_train, y_train)\n",
    "svm_pred = svm_model.predict(df_test)\n",
    "print(f\"SVM (Linear) Accuracy: {accuracy_score(y_test, svm_pred):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate SVM with RBF kernel\n",
    "svm_rbf_model = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf_model.fit(df_train, y_train)\n",
    "svm_rbf_pred = svm_rbf_model.predict(df_test)\n",
    "print(f\"SVM (RBF) Accuracy: {accuracy_score(y_test, svm_rbf_pred):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, svm_rbf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=300, random_state=42, criterion='entropy')\n",
    "rf_model.fit(df_train, y_train)\n",
    "rf_pred = rf_model.predict(df_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert sequence numbers back to words\n",
    "def extract_and_concatenate(string, word_dict):\n",
    "    numbers = re.findall(r'\\((\\d+)\\)|\\[(\\d+)\\]', string)\n",
    "    numbers = [int(num) for pair in numbers for num in pair if num]\n",
    "    words = [word_dict.get(num, \"\") for num in numbers]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences to readable words for analysis\n",
    "all_sequences_words_human = []\n",
    "for i in sequences_human:\n",
    "    string = extract_and_concatenate(str(i), index_to_word)\n",
    "    all_sequences_words_human.append(string)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
