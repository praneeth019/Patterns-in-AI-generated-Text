{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Uncomment these lines when using Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "equal_sample_df = pd.read_csv('path to data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Helper functions for text processing\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove all punctuation from text.\"\"\"\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "def generate_ngrams_range(text, n_start=2, n_end=2):\n",
        "    \"\"\"Generate n-grams for a range of n values.\"\"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    all_ngrams = []\n",
        "    for n in range(n_start, n_end + 1):\n",
        "        n_grams = list(ngrams(tokens, n))\n",
        "        all_ngrams.extend([' '.join(gram) for gram in n_grams])\n",
        "    return all_ngrams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Separate the data by label and create balanced samples\n",
        "human_data = equal_sample_df[equal_sample_df[\"source\"] == 0]\n",
        "ai_data = equal_sample_df[equal_sample_df[\"source\"] == 1]\n",
        "\n",
        "# Downsample to ensure equal representation (5000 samples each)\n",
        "human_sampled = human_data.sample(n=5000, random_state=42)\n",
        "ai_data = ai_data.sample(n=5000, random_state=42)\n",
        "\n",
        "# Preprocess AI data\n",
        "ai_data[\"pos_tags\"] = ai_data['pos_tags'].apply(remove_punctuation)\n",
        "ai_data[\"ngrams\"] = ai_data['pos_tags'].apply(lambda x: generate_ngrams_range(x, n_start=2, n_end=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create Word2Vec model from the n-grams\n",
        "model = Word2Vec(\n",
        "    ai_data['ngrams'].to_list(),\n",
        "    vector_size=50,\n",
        "    window=2,\n",
        "    min_count=3,\n",
        "    sg=0,\n",
        "    max_vocab_size=100000\n",
        ")\n",
        "\n",
        "# Extract vectors for each n-gram\n",
        "vectors = model.wv.vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Perform hierarchical clustering with Ward's method\n",
        "linkage_matrix = linkage(vectors, method='ward')\n",
        "\n",
        "# Define the distance threshold for clusters\n",
        "threshold = 10\n",
        "cluster_labels = fcluster(linkage_matrix, threshold, criterion='distance')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the n-grams list for analysis\n",
        "ngrams_list = ai_data['ngrams'].tolist()\n",
        "\n",
        "# Filter ngrams_list to align with Word2Vec vocabulary\n",
        "filtered_ngrams_list = [ngrams for ngrams in ngrams_list if any(ngram in model.wv for ngram in ngrams)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Ensure cluster_labels and filtered_ngrams_list have the same length\n",
        "cluster_labels = cluster_labels[:len(filtered_ngrams_list)]\n",
        "\n",
        "# Organize n-grams by cluster\n",
        "clusters = {i: [] for i in set(cluster_labels)}\n",
        "for i, label in enumerate(cluster_labels):\n",
        "    clusters[label].append(filtered_ngrams_list[i])\n",
        "\n",
        "# Analyze clusters to extract features\n",
        "selected_clusters = pd.Series(cluster_labels).value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the top 20 and bottom 10 clusters\n",
        "top_clusters = selected_clusters.head(20).index.tolist()\n",
        "bottom_clusters = selected_clusters.tail(10).index.tolist()\n",
        "clusters_to_process = top_clusters + bottom_clusters\n",
        "\n",
        "# Initialize storage for features\n",
        "features = []\n",
        "unique_features = set()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Process clusters to extract meaningful features\n",
        "for cluster_id in clusters_to_process:\n",
        "    # Compute mean values for the cluster\n",
        "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
        "\n",
        "    # Filter ai_data['ngrams'] based on cluster indices\n",
        "    cluster_ngrams = [ai_data['ngrams'].iloc[i] for i in cluster_indices]\n",
        "\n",
        "    # Flatten the list of ngrams for the cluster\n",
        "    flat_cluster_ngrams = [ngram for sublist in cluster_ngrams for ngram in sublist]\n",
        "\n",
        "    # Convert ngrams to vectors using the Word2Vec model\n",
        "    cluster_vectors = [model.wv[ngram] for ngram in flat_cluster_ngrams if ngram in model.wv]\n",
        "\n",
        "    # Calculate cluster mean if cluster_vectors is not empty\n",
        "    if cluster_vectors:\n",
        "        cluster_mean = np.mean(cluster_vectors, axis=0)\n",
        "\n",
        "        # Get top and bottom indices\n",
        "        top_indices = np.argsort(cluster_mean)[-40:][::-1]  # Top 40 features\n",
        "        bottom_indices = np.argsort(cluster_mean)[:40]      # Bottom 40 features\n",
        "\n",
        "        # Add features to storage\n",
        "        top_features = [model.wv.index_to_key[i] for i in top_indices]\n",
        "        top_clusters = top_features\n",
        "        unique_features.update(top_features)\n",
        "\n",
        "        bottom_features = [model.wv.index_to_key[i] for i in bottom_indices]\n",
        "        bottom_clusters = bottom_features\n",
        "        unique_features.update(bottom_features)\n",
        "    else:\n",
        "        print(f\"Cluster {cluster_id} has no ngrams in the Word2Vec model vocabulary.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
